{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of neural_style_transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohit9650/ML/blob/master/Neural%20Style%20Transfer/neural_style_transfer_presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "quC4xomwIrAI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Neural Style Transfer**\n",
        "<center>![](https://raw.githubusercontent.com/rohit9650/ML/master/Neural%20Style%20Transfer/3.png)\n",
        "</center>\n",
        "\n",
        "\n",
        "## <center>By Rohit Singh</center>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XYebwKFcC7QG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#*Motivation*\n",
        "\n",
        "\n",
        "\n",
        "1.   Prisma\n",
        "2.   Representation Learning\n",
        "3.   A Neural Algorithm of Artistic Style\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Rnhwi2zPD7ML",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#*Overview*\n",
        "![alt text](https://raw.githubusercontent.com/rohit9650/ML/master/Neural%20Style%20Transfer/4.png)\n",
        "\n",
        "Neural style transfer consists of applying the style of a reference image to a target\n",
        "image while conserving the content of the target image.\n",
        "\n",
        "In this context, \n",
        "**style** *essentially means textures, colors, and visual patterns in the image, at\n",
        "various spatial scales*; \n",
        "and the** content** *is the higher-level macrostructure of the image.*\n",
        "\n",
        "For instance, blue-and-yellow circular brushstrokes are considered to be the style in (using Starry Night by Vincent Van Gogh), and the buildings in the photograph are considered to be the content.\n",
        "\n",
        "\n",
        "#*Idea*\n",
        "\n",
        "The key notion behind implementing style transfer is the same idea that’s central\n",
        "to all deep-learning algorithms: you define a loss function to specify what you want to\n",
        "achieve, and you minimize this loss.\n",
        "\n",
        "\n",
        "##So what's the loss ?\n",
        "\n",
        "####conserving the content of the original image while adopting the style of the reference image.\n",
        "\n",
        "\n",
        "\n",
        "> loss = distance(style(reference_image) - style(generated_image)) + \n",
        ">> distance(content(original_image) - content(generated_image))\n",
        "\n",
        "\n",
        "##Need a neural network\n",
        "\n",
        ">we'll use VGG19\n",
        "\n",
        "\n",
        "#Approach \n",
        "Neural style transfer can be implemented using any pretrained convnet. Here, you’ll\n",
        "use the VGG19 network used by Gatys et al. VGG19 is a simple variant of the VGG16 net-\n",
        "work introduced in chapter 5, with three more convolutional layers.\n",
        "This is the general process:\n",
        "\n",
        "\n",
        "1.   Set up a network that computes VGG19 layer activations for the style-reference\n",
        "image, the target image, and the generated image at the same time.\n",
        "2.   Use the layer activations computed over these three images to define the loss\n",
        "function described earlier, which you’ll minimize in order to achieve style\n",
        "transfer.\n",
        "3.    Set up a gradient-descent process to minimize this loss function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "E061st-YfDld",
        "colab_type": "code",
        "outputId": "df78bdd3-d58b-4c24-e548-4ec7ff882389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Keras implementation of the original 2015 neural style transfer\n",
        "algorithm.\n",
        "Neural style transfer can be implemented using any pretrained convnet. \n",
        "Here, we’ll use the VGG19 network used by Gatys et al. \n",
        "VGG19 is a simple variant of the VGG16 network.\n",
        "'''\n",
        "! wget -O style.jpeg https://cdn-images-1.medium.com/max/2000/1*yOmBkzDKE6HuWOvrWw1E1Q.jpeg \n",
        "! wget -O target.jpeg http://s1.bwallpapers.com/wallpapers/2014/01/03/sherlock-wallpapers_095314.jpg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-02-09 21:06:22--  https://cdn-images-1.medium.com/max/2000/1*yOmBkzDKE6HuWOvrWw1E1Q.jpeg\n",
            "Resolving cdn-images-1.medium.com (cdn-images-1.medium.com)... 104.16.120.145, 104.16.118.145, 104.16.121.145, ...\n",
            "Connecting to cdn-images-1.medium.com (cdn-images-1.medium.com)|104.16.120.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 678193 (662K) [image/jpeg]\n",
            "Saving to: ‘style.jpeg’\n",
            "\n",
            "\rstyle.jpeg            0%[                    ]       0  --.-KB/s               \rstyle.jpeg          100%[===================>] 662.30K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-02-09 21:06:22 (20.0 MB/s) - ‘style.jpeg’ saved [678193/678193]\n",
            "\n",
            "--2019-02-09 21:06:24--  http://s1.bwallpapers.com/wallpapers/2014/01/03/sherlock-wallpapers_095314.jpg\n",
            "Resolving s1.bwallpapers.com (s1.bwallpapers.com)... 139.99.9.201\n",
            "Connecting to s1.bwallpapers.com (s1.bwallpapers.com)|139.99.9.201|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 391378 (382K) [image/jpeg]\n",
            "Saving to: ‘target.jpeg’\n",
            "\n",
            "target.jpeg         100%[===================>] 382.21K  78.8KB/s    in 4.8s    \n",
            "\n",
            "2019-02-09 21:06:29 (78.8 KB/s) - ‘target.jpeg’ saved [391378/391378]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pwUDuFjJipg1",
        "colab_type": "code",
        "outputId": "0506410d-61c1-4efd-c965-ec8cb7012a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Defining initial variables\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "target_image_path = 'target.jpeg'\n",
        "style_reference_image_path = 'style.jpeg'\n",
        "\n",
        "width, height = load_img(target_image_path).size\n",
        "\n",
        "img_height = 400\n",
        "img_width = int(width * img_height / height)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "C9RNfaGNKB9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Pre-processing and Deprocessing\n",
        "\n",
        "##pre-processing\n",
        ">Don't need much, vgg19 gonna handle it.\n",
        "\n",
        "##Deprocessing\n",
        "> vgg19 has zero centered the image, so need to add mean of each dimension \n",
        "> also it has changed the image to BGR."
      ]
    },
    {
      "metadata": {
        "id": "ZhLkjALnm2G7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "import numpy as np\n",
        "from keras.applications import vgg19\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  img = load_img(image_path, target_size=(img_height, img_width))\n",
        "  img = img_to_array(img)\n",
        "  img = np.expand_dims(img,axis=0)\n",
        "  img = vgg19.preprocess_input(img)\n",
        "  \n",
        "  return img\n",
        "\n",
        "def deprocess_img(x):\n",
        "  '''\n",
        "  Zero-centering by removing the mean pixel value\n",
        "  from ImageNet. This reverses a transformation\n",
        "  done by vgg19.preprocess_input.\n",
        "  \n",
        "  Converts images from 'BGR' to 'RGB'.\n",
        "  This is also part of the reversal of\n",
        "  vgg19.preprocess_input.\n",
        "  '''\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  \n",
        "  x = x[:, :, ::-1]\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  \n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OKA9MT-NOzXf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Set up VGG19\n",
        "\n",
        "It takes as input a batch of three images: \n",
        "1.    the style-reference image\n",
        "2.    the target image and \n",
        "3.    a placeholder that will contain the generated image.\n",
        "\n",
        "A placeholder is a symbolic tensor, the values of which are provided externally\n",
        "via Numpy arrays. The style-reference and target image are static and thus defined\n",
        "using K.constant , whereas the values contained in the placeholder of the generated\n",
        "image will change over time"
      ]
    },
    {
      "metadata": {
        "id": "AfsLufLCozcq",
        "colab_type": "code",
        "outputId": "7248c8b1-04fa-48a2-bbf7-80b9307a0571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading the pretrained VGG19 network and applying it to the three images\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "target_image = K.constant(preprocess_image(target_image_path))\n",
        "style_reference_image = K.constant(preprocess_image(\n",
        "    style_reference_image_path))\n",
        "\n",
        "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
        "\n",
        "input_tensor = K.concatenate([target_image, \n",
        "                             style_reference_image, \n",
        "                             combination_image], axis=0)\n",
        "\n",
        "model = vgg19.VGG19(input_tensor=input_tensor, \n",
        "                    weights='imagenet', \n",
        "                    include_top=False)\n",
        "\n",
        "print('Model Loaded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 2s 0us/step\n",
            "Model Loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UbXdL1EVQBne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Loss Function\n",
        "\n",
        "##*Content Loss*\n",
        "![alt text](https://raw.githubusercontent.com/rohit9650/ML/master/Neural%20Style%20Transfer/content_loss.png)\n",
        "\n",
        "##*Style Loss*\n",
        "\n",
        " while calculating the style loss we will consider feature representation of many convolution layers from shallow to deeper layers of the model.\n",
        " \n",
        " ### *But Wait !!!*\n",
        " Unlike content loss we can’t just find the difference in activation units.\n",
        " \n",
        " ### *What Should we do ?*\n",
        " What we need is a way to find the correlation between these activations across different channels of the same layer and to do this we need something called as the Gram Matrix.\n",
        " \n",
        " ### *Gram Matrix*\n",
        " ![alt text](https://cdn-images-1.medium.com/max/800/0*p0RFEgFg_zP-J6kD)\n",
        " ![alt text](https://cdn-images-1.medium.com/max/800/0*u4U60W2sblmoCtv1)\n",
        " \n",
        "Each element of this gram matrix contains correlation measure of all the channels with respect to each other.\n",
        "\n",
        "### *how do we use this computed Gram matrix G to calculate the style loss*\n",
        "Let’s denote the gram matrix of style image of layer L as GM[L] (S) and gram matrix of generated image of same layer as GM[L] (G). Both the gram matrix were computed from the same layer hence using the same number of channel leading it to be a matrix of size ch x ch, Now if we find sum of square difference or L2_norm of element subtraction of these two matrices and try to minimize it, Then this will eventually lead to minimizing the difference between the style of style image and the generated image. \n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*IoozR3xGzaSqtEqGEKcWMQ.jpeg)\n",
        "\n",
        "In the above equation, *N* subscript *l* represents the number of channel in the feature-map/output of layer *l* and *M* subscript *l* represents the height x width of the feature-map/output of layer *l*.\n",
        "\n",
        "## *We can add total variation loss*\n",
        "It operates on the pixels of the generated combination image. It encourages spatial continuity in\n",
        "the generated image, thus avoiding overly pixelated results. We can interpret it as a\n",
        "regularization loss."
      ]
    },
    {
      "metadata": {
        "id": "MPz4HCMGqxX-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Content loss\n",
        "\n",
        "def content_loss(base, combination):\n",
        "  return K.sum(K.square(combination - base))\n",
        "\n",
        "# Style loss\n",
        "\n",
        "def gram_matrix(x):\n",
        "  features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "  gram = K.dot(features, K.transpose(features))\n",
        "  \n",
        "  return gram\n",
        "\n",
        "def style_loss(style, combination):\n",
        "  S = gram_matrix(style)\n",
        "  C = gram_matrix(combination)\n",
        "  channels = 3\n",
        "  size = img_height*img_width\n",
        "  \n",
        "  return K.sum(K.square(S-C)) / (4. * (channels ** 2) * (size ** 2))\n",
        "\n",
        "# Total variation loss\n",
        "\n",
        "def total_variation_loss(x):\n",
        "  '''\n",
        "  It operates on the pixels of the generated combination image. \n",
        "  It encourages spatial continuity in the generated image, thus avoiding \n",
        "  overly pixelated results. You can interpret it as a regularization loss.\n",
        "  '''\n",
        "  a = K.square(\n",
        "      x[:, :img_height-1, :img_width-1, :] - \n",
        "      x[:, 1:, :img_width-1, :])\n",
        "  b = K.square(\n",
        "      x[:, :img_height-1, :img_width-1, :] - \n",
        "      x[:, :img_height-1, 1:, :])\n",
        "  \n",
        "  return K.sum(K.pow(a+b, 1.25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A77z4bWHXWz1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "jvEbp7PbrD5m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Defining the final loss that is needed to be minimized\n",
        "\n",
        "outputs_dict = dict([(layer.name, layer.output) \n",
        "                     for layer in model.layers])\n",
        "\n",
        "content_layer = 'block5_conv2'\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1', \n",
        "                'block3_conv1', \n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "\n",
        "total_variation_weight = 1e-4\n",
        "style_weight = 1.\n",
        "content_weight = 0.025\n",
        "\n",
        "loss = K.variable(0.)\n",
        "\n",
        "layer_features = outputs_dict[content_layer]\n",
        "target_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss = loss + content_weight * content_loss(target_image_features,\n",
        "                                            combination_features)\n",
        "\n",
        "for layer_name in style_layers:\n",
        "  layer_features = outputs_dict[layer_name]\n",
        "  style_reference_features = layer_features[1, :, :, :]\n",
        "  combination_features = layer_features[2, :, :, :]\n",
        "  sl = style_loss(style_reference_features, combination_features)\n",
        "  loss = loss + (style_weight / len(style_layers)) * sl\n",
        "\n",
        "loss = loss + total_variation_weight * total_variation_loss(\n",
        "    combination_image)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IMjtLRyKbBb2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setting up the gradient-descent process\n",
        "\n",
        "Finally, we’ll set up the gradient-descent process. In the original \n",
        "Gatys et al. paper, optimization is performed using the [L-BFGS algorithm](https://en.wikipedia.org/wiki/Limited-memory_BFGS), \n",
        "so that’s what we’ll use here.\n",
        "The L-BFGS algorithm comes packaged with SciPy, but there are two slight\n",
        "limitations with the SciPy implementation:\n",
        "\n",
        "* It requires that you pass the value of the loss function and the value \n",
        "  of the gradients as two separate functions.\n",
        "* It can only be applied to flat vectors, whereas we have a 3D img array.\n",
        "\n",
        "It would be inefficient to compute the value of the loss function and \n",
        "the value of the gradients independently, because doing so would lead\n",
        "to a lot of redundant computation between the two; the process would be \n",
        "almost twice as slow as computing them jointly. To bypass this, we’ll set\n",
        "up a Python class named Evaluator that computes both the loss value \n",
        "and the gradients value at once, returns the loss value when called\n",
        "the first time, and caches the gradients for the next call.\n"
      ]
    },
    {
      "metadata": {
        "id": "gbzn9ZsX0Of4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "grads = K.gradients(loss, combination_image)[0]\n",
        "\n",
        "fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
        "\n",
        "class Evaluator(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.loss_value = None\n",
        "    self.grads_values = None\n",
        "  \n",
        "  def loss(self, x):\n",
        "    assert self.loss_value is None\n",
        "    \n",
        "    x = x.reshape((1, img_height, img_width, 3))\n",
        "    outs = fetch_loss_and_grads([x])\n",
        "    \n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1].flatten().astype('float64')\n",
        "    \n",
        "    self.loss_value = loss_value\n",
        "    self.grad_values = grad_values\n",
        "    \n",
        "    return self.loss_value\n",
        "  \n",
        "  def grads(self, x):\n",
        "    assert self.loss_value is not None\n",
        "    \n",
        "    grad_values = np.copy(self.grad_values)\n",
        "    self.loss_value = None\n",
        "    self.grad_values = None\n",
        "    \n",
        "    return grad_values\n",
        "\n",
        "evaluator = Evaluator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bUFuhVbOcBJH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Style-transfer loop\n",
        "\n",
        "Finally, we can run the gradient-ascent process using SciPy’s  L-BFGS \n",
        "algorithm, saving the current generated image at each iteration of the \n",
        "algorithm (a single iteration represents 20 steps of gradient ascent).\n"
      ]
    },
    {
      "metadata": {
        "id": "vyAZF2UI_rga",
        "colab_type": "code",
        "outputId": "fda35303-ef5f-48fe-ed54-6ffa53e3eec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1531
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from scipy.misc import imsave\n",
        "import time\n",
        "\n",
        "result_prefix = 'my_result'\n",
        "iterations = 20\n",
        "\n",
        "x = preprocess_image(target_image_path)\n",
        "x = x.flatten()\n",
        "\n",
        "for i in range(iterations):\n",
        "  print('start of iteration : ', i)\n",
        "  start_time = time.time()\n",
        "  x, min_val, info = fmin_l_bfgs_b(evaluator.loss, \n",
        "                                   x,\n",
        "                                   fprime=evaluator.grads,\n",
        "                                   maxfun=20)\n",
        "  print('current loss value : ', min_val)\n",
        "  img = x.copy().reshape((img_height, img_width, 3))\n",
        "  img = deprocess_img(img)\n",
        "  fname = result_prefix + '_at_iteration_%d.png' % i\n",
        "  imsave(fname, img)\n",
        "  \n",
        "  print('image saved as : ', fname)\n",
        "  end_time = time.time()\n",
        "  print('Iteration %d completed in %ds' % (i, end_time - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start of iteration :  0\n",
            "current loss value :  1340980700.0\n",
            "image saved as :  my_result_at_iteration_0.png\n",
            "Iteration 0 completed in 20s\n",
            "start of iteration :  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "current loss value :  590207170.0\n",
            "image saved as :  my_result_at_iteration_1.png\n",
            "Iteration 1 completed in 20s\n",
            "start of iteration :  2\n",
            "current loss value :  369717660.0\n",
            "image saved as :  my_result_at_iteration_2.png\n",
            "Iteration 2 completed in 20s\n",
            "start of iteration :  3\n",
            "current loss value :  282130660.0\n",
            "image saved as :  my_result_at_iteration_3.png\n",
            "Iteration 3 completed in 20s\n",
            "start of iteration :  4\n",
            "current loss value :  239042020.0\n",
            "image saved as :  my_result_at_iteration_4.png\n",
            "Iteration 4 completed in 20s\n",
            "start of iteration :  5\n",
            "current loss value :  194469820.0\n",
            "image saved as :  my_result_at_iteration_5.png\n",
            "Iteration 5 completed in 20s\n",
            "start of iteration :  6\n",
            "current loss value :  169658530.0\n",
            "image saved as :  my_result_at_iteration_6.png\n",
            "Iteration 6 completed in 20s\n",
            "start of iteration :  7\n",
            "current loss value :  150540820.0\n",
            "image saved as :  my_result_at_iteration_7.png\n",
            "Iteration 7 completed in 20s\n",
            "start of iteration :  8\n",
            "current loss value :  137065550.0\n",
            "image saved as :  my_result_at_iteration_8.png\n",
            "Iteration 8 completed in 20s\n",
            "start of iteration :  9\n",
            "current loss value :  126567880.0\n",
            "image saved as :  my_result_at_iteration_9.png\n",
            "Iteration 9 completed in 21s\n",
            "start of iteration :  10\n",
            "current loss value :  115817784.0\n",
            "image saved as :  my_result_at_iteration_10.png\n",
            "Iteration 10 completed in 20s\n",
            "start of iteration :  11\n",
            "current loss value :  109427060.0\n",
            "image saved as :  my_result_at_iteration_11.png\n",
            "Iteration 11 completed in 21s\n",
            "start of iteration :  12\n",
            "current loss value :  102817670.0\n",
            "image saved as :  my_result_at_iteration_12.png\n",
            "Iteration 12 completed in 20s\n",
            "start of iteration :  13\n",
            "current loss value :  97610390.0\n",
            "image saved as :  my_result_at_iteration_13.png\n",
            "Iteration 13 completed in 20s\n",
            "start of iteration :  14\n",
            "current loss value :  92193750.0\n",
            "image saved as :  my_result_at_iteration_14.png\n",
            "Iteration 14 completed in 21s\n",
            "start of iteration :  15\n",
            "current loss value :  88499230.0\n",
            "image saved as :  my_result_at_iteration_15.png\n",
            "Iteration 15 completed in 20s\n",
            "start of iteration :  16\n",
            "current loss value :  84582310.0\n",
            "image saved as :  my_result_at_iteration_16.png\n",
            "Iteration 16 completed in 21s\n",
            "start of iteration :  17\n",
            "current loss value :  81337064.0\n",
            "image saved as :  my_result_at_iteration_17.png\n",
            "Iteration 17 completed in 20s\n",
            "start of iteration :  18\n",
            "current loss value :  77603500.0\n",
            "image saved as :  my_result_at_iteration_18.png\n",
            "Iteration 18 completed in 21s\n",
            "start of iteration :  19\n",
            "current loss value :  74640750.0\n",
            "image saved as :  my_result_at_iteration_19.png\n",
            "Iteration 19 completed in 20s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}